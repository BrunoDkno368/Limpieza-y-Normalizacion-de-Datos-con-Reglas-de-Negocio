ğŸ“Š Proyecto ETL con Python y Pandas â€“ Limpieza y NormalizaciÃ³n de Datos Bancarios
ğŸ“Œ DescripciÃ³n general

Este proyecto implementa un proceso ETL (Extract, Transform, Load) completo utilizando Python, Pandas y expresiones regulares, aplicado a un dataset bancario con datos sucios, inconsistentes y no estructurados.

El objetivo principal es transformar datos crudos en un dataset limpio, consistente y apto para anÃ¡lisis, respetando reglas de negocio reales del sector bancario.

El proyecto fue desarrollado en Google Colab y estÃ¡ orientado a demostrar habilidades prÃ¡cticas en:

Data Cleaning

Data Quality

Validaciones de negocio

Transformaciones complejas

Pensamiento analÃ­tico aplicado a ETL

ğŸ§° TecnologÃ­as utilizadas

Python

Pandas

NumPy

Regex (re)

Google Colab

ğŸ—‚ï¸ Estructura del proceso ETL

1ï¸âƒ£ Extract (E)

Lectura de un archivo CSV con mÃºltiples inconsistencias:

Tipos de datos incorrectos

Valores nulos

Errores semÃ¡nticos

Formatos mixtos

Valores fuera de rango

df = pd.read_csv('banco_dataset_sucio.csv')

2ï¸âƒ£ Transform (T)

La etapa principal del proyecto.
Cada columna fue tratada de forma independiente, aplicando reglas de negocio explÃ­citas.

ğŸ”¹ Limpieza y decisiones por columna

ğŸ†” id_cliente

Reglas de negocio

No nulo

Ãšnico

Tipo entero

Decisiones

EliminaciÃ³n de registros nulos

ConversiÃ³n explÃ­cita a int

VerificaciÃ³n de duplicados

Motivo
Clave primaria del dataset. No se permite ambigÃ¼edad ni pÃ©rdida de integridad.

ğŸ‘¤ nombre

Problemas detectados

SÃ­mbolos especiales (@, !)

Espacios mal formateados

Valores nulos

Palabras cortadas (Mar ia)

Acciones

ConversiÃ³n a StringDtype

Limpieza con regex

NormalizaciÃ³n de espacios

Correcciones semÃ¡nticas

CapitalizaciÃ³n

ImputaciÃ³n controlada: "Sin datos"

Motivo
Garantizar consistencia textual sin eliminar registros.

ğŸ‚ edad

Reglas

Rango vÃ¡lido: 18â€“100

No texto

No valores negativos

Regla bancaria: no se opera con menores

Acciones

Mapeo semÃ¡ntico (treinta â†’ 30)

ConversiÃ³n a numÃ©rico con errors='coerce'

Valores fuera de rango â†’ NaN

ImputaciÃ³n con mediana

ConversiÃ³n final a entero

Motivo
Evitar sesgos y mantener coherencia estadÃ­stica.

ğŸ“§ email

Reglas

Debe contener @ y dominio vÃ¡lido

Emails invÃ¡lidos â†’ NaN

Emails duplicados permitidos

Acciones

NormalizaciÃ³n a minÃºsculas

ValidaciÃ³n con regex

Uso de na=False para evitar errores

ImputaciÃ³n con "Sin datos"

Motivo
Campo informativo, no clave. Se prioriza limpieza sobre eliminaciÃ³n.

ğŸ’° saldo_cuenta

Reglas

No texto

No negativo

MÃ¡ximo: 10.000.000

Problemas

Separadores mixtos (. y ,)

Valores extremos

Texto (abc)

Acciones

NormalizaciÃ³n de separadores

ConversiÃ³n a numÃ©rico

ValidaciÃ³n de rango

Outliers â†’ NaN

ImputaciÃ³n con mediana

Motivo
Evitar distorsiÃ³n financiera y preservar distribuciÃ³n.

ğŸ¦ tipo_cuenta

Valores permitidos

Caja Ahorro

Cuenta Corriente

Acciones

NormalizaciÃ³n a minÃºsculas

Mapeo semÃ¡ntico (CA, CC, CajaAhorro, etc.)

Valores desconocidos â†’ "Sin Datos"

CapitalizaciÃ³n final

Motivo
EstandarizaciÃ³n categÃ³rica para anÃ¡lisis y reporting.

ğŸ“… fecha_alta

Problemas

MÃºltiples formatos

Fechas invÃ¡lidas

Fechas inexistentes

Texto con meses en espaÃ±ol

Acciones

TraducciÃ³n de meses

Parsing mÃºltiple

Correcciones estructurales

ConversiÃ³n a datetime

Fechas invÃ¡lidas â†’ NaT

ImputaciÃ³n controlada

Motivo
Preservar la mayor cantidad de fechas vÃ¡lidas posibles sin errores silenciosos.

ğŸš¦ estado_cliente

Valores permitidos

Activo

Inactivo

Suspendido

Acciones

NormalizaciÃ³n de texto

CapitalizaciÃ³n

Valores nulos â†’ Inactivo / Sin Datos

Motivo
Campo categÃ³rico clave para segmentaciÃ³n.

3ï¸âƒ£ Load (L)

ExportaciÃ³n del dataset limpio para anÃ¡lisis posterior:

df.to_csv('datos_limpios.csv', index=False)

â“ Preguntas clave surgidas durante el proceso

Durante el desarrollo del ETL surgieron preguntas tÃ©cnicas relevantes que guiaron decisiones de diseÃ±o:

Â¿CuÃ¡ndo usar astype(str) vs astype("string")?

Â¿Por quÃ© usar na=False en validaciones con .str.match()?

Â¿CuÃ¡ndo imputar con media y cuÃ¡ndo con mediana?

Â¿Es correcto reemplazar datos invÃ¡lidos o es mejor eliminarlos?

Â¿CÃ³mo manejar fechas con mÃºltiples formatos sin perder informaciÃ³n?

Â¿CÃ³mo evitar errores silenciosos en parsing de fechas?

Estas preguntas fueron fundamentales para aplicar buenas prÃ¡cticas de Data Engineering.

ğŸ¯ Conclusiones

Este proyecto demuestra:

Capacidad para diseÃ±ar ETL de punta a punta

AplicaciÃ³n de reglas de negocio reales

Uso correcto de Pandas avanzado

Pensamiento crÃ­tico en decisiones de limpieza

Enfoque profesional orientado a Data Analyst / Data Engineer Jr


ğŸ‘¤ Autor
Bruno ArgaÃ±araz Analista de Datos TucumÃ¡n, Argentina

LinkedIn: https://www.linkedin.com/in/bruno-arga%C3%B1araz-726a4a199/
ğŸ”‘

ETL, Python, Pandas, Data Cleaning, Data Quality,
Data Analyst, Data Engineering, Regex, Business Rules,
Data Transformation, Data Wrangling, Google Colab,
CSV Processing, Banking Dataset
